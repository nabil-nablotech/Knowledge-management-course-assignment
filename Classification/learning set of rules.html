<!DOCTYPE html>
<html>
    <head>
        <title>Classification- Learning set of rules</title>
        <link rel="stylesheet" href="../style.css">
    </head>
    <body>
        <div id="header">
            <ul>
                <li ><a href="../home.html">Home</a></li>
                <li class="dropdown" >Description
                    <div class="dropmenu">
                        <span class="sidedropdown">Graphical Methods &#8250;
                            <div class="sidemenu">
                                <a href="../description/run chart.html"><span>Run charts</span></a>
                                <a href="../description/order chart.html"><span>Order charts</span></a>
                                <a href="../description/special chart.html"><span>Special charts</span></a>
                            </div>
                        </span>
                        <span class="sidedropdown">Statistical Methods &#8250;
                            <div class="sidemenu">
                                <a href="../description/hypothesis testing.html"><span>Hypothesis testing</span></a>
                                <a href="../description/coorelation.html"><span>Coorelation data</span></a>
                                <a href="../description/multi corelation data.html"><span>Multi corelation data</span></a>
                            </div>
                        </span>
                        <a href="../description/exploratory data analysis.html"><span>Exploratory Data Analysis</span></a>
                    </div>
                </li>

                <li class="dropdown" style="background-color: #30475e;">Classification
                    <div class="dropmenu">
                        <span class="sidedropdown">Logic based algorithim &#8250;
                            <div class="sidemenu">
                                <a href="../Classification/decision tree.html"><span>Decision trees</span></a>
                                <a href="../Classification/learning set of rules.html"><span>Learning set of rules</span></a>
                            </div>
                        </span>
                        <span class="sidedropdown">Perception based learners &#8250;
                            <div class="sidemenu">
                                <a href="../Classification/single layered perception.html"><span>Single layered perception</span></a>
                                <a href="../Classification/Multi layered perception.html"><span>Multi layered perception</span></a>
                                <a href="../Classification/Radial basis function network.html"><span>Radial basis function networks</span></a>
                            </div>
                        </span>
                        <span class="sidedropdown">Statistical learners &#8250;
                            <div class="sidemenu">
                                <a href="../Classification/naive bayes classifier.html"><span>Naive Bayes classifier</span></a>
                                <a href="../Classification/bayesian network.html"><span>Bayesian networks</span></a>
                            </div>
                        </span>
                        <span class="sidedropdown">Instance-based learners &#8250;
                            <div class="sidemenu">
                                <a href="../Classification/k-nearest.html"><span>k-nearest neighbours</span></a>
                            </div>
                        </span>
                        <a href="../Classification/simple vector machine.html"><span>Support Vector Machines</span></a>
                    </div>
                </li>

                <li class="dropdown">Regression
                    <div class="dropmenu">
                        <a href="#"><span >Linear regression</span></a>
                        <a href="#"><span >Polynomial regression</span></a>
                        <a href="#"><span >Multiple linear regression</span></a>
                        <span class="sidedropdown">Generalized linear models &#8250;
                            <div class="sidemenu">
                                <a href="#"><span>Logic regression</span></a>
                                <a href="#"><span>Poisson regression</span></a>
                            </div>
                        </span>
                        <a href="#"><span >Log-linear models</span></a>
                        <a href="#"><span >Regression trees</span></a>
                        <a href="#"><span >Instance-based regression</span></a>
                        <a href="#"><span>Multivariate adaptive regression splines</span></a>
                        <a href="#"><span>Support Vector Machines</span></a>
                        <a href="#"><span >Neural networks</span></a>
                    </div>
                </li>

                <li class="dropdown">Clustering
                    <div class="dropmenu">
                        <span class="sidedropdown">Hierarchical methods &#8250;
                            <div class="sidemenu">
                                <a href="#"><span>Agglomerative</span></a>
                                <a href="#"><span>Divisive</span></a>
                            </div>
                        </span>
                        <span class="sidedropdown">Partitioning methods &#8250;
                            <div class="sidemenu">
                                <a href="#"><span>K-Means</span></a>
                                <a href="#"><span>K-Medoids</span></a>
                            </div>
                        </span>
                        <span class="sidedropdown">Density-based methods &#8250;
                            <div class="sidemenu">
                                <a href="#"><span>DBSCAN</span></a>
                                <a href="#"><span>OPTICS</span></a>
                                <a href="#"><span>DENCLUE</span></a>
                            </div>
                        </span>
                        <span class="sidedropdown">Grid-based methods &#8250;
                            <div class="sidemenu">
                                <a href="#"><span>STING</span></a>
                                <a href="#"><span>WaveCluster</span></a>
                                <a href="#"><span>CLIQUE</span></a>
                            </div>
                        </span>
                        <span class="sidedropdown">Model base methods &#8250;
                            <div class="sidemenu">
                                <a href="#"><span>EM method</span></a>
                                <a href="#"><span>Conceptual Clustering</span></a>
                                <a href="#"><span>Neural networks</span></a>
                            </div>
                        </span>
                    </div>
                </li>

                <li class="dropdown">Association
                    <div class="dropmenu">
                        <a href="#"><span >Apriori algorithim</span></a>
                        <a href="#"><span >FP-Growth algorithim</span></a>
                        <a href="#"><span >Dynamic Hashing and Pruning</span></a>
                        <a href="#"><span >Partition algorithim</span></a>
                        <a href="#"><span >Sampling based algorithim</span></a>
                        <a href="#"><span >Dynamic Itemset</span></a>
                        <a href="#"><span >Counting tree-projection</span></a>
                    </div>
                </li>
                
            </ul>
        </div>
        <div id="content">
            <h1>Learning set of rules</h1>
            <p style="font-size: 12px;">Classification / Logic based algorithim / Learning set of rules</p>
            
            <br>
            <ol>
                <li>
                    <h3>Learning Rules</h3>
                    <ul>
                        <li>We can learn sets of rules by using ID3 and then converting the tree to rules.</li>
                        <li>We can also use a genetic algorithm that encodes the rules as bit strings.</li>
                        <li>But, these only work with predicate rules (no variables).</li>
                        <li>They also consider the set of rules as a whole, not one rule at a time.</li>
                    </ul>
                </li>
                <li>
                    <h3>Rules</h3>
                    <ul>
                        <li>First-order predicate logic (calculus) [3] formalizes statements using predicates (boolean functions) and functions. Both can have variables.</li>
                        <li>A rule set can look like
                            <p>
                                <img src="learning set of rules.jpg" alt="learning set of rules">
                            </p>
                        </li>
                        <li>Here, Parent(x,y) is a predicate that indicates that y is the parent of x.</li>
                        <li>These two rules form a recursive function which would be very hard to represent using a decision tree or propositional representation.</li>
                        <li>In Prolog [4], programs are set of first-order rules with the form as above (known as Horn clauses).</li>
                        <li>We can view the learning of rules as the learning of Prolog programs.</li>
                    </ul>
                </li>
                <li>
                    <h3>Sequential Covering</h3>
                    <ul>
                        <li>
                            <p>
                                The idea in a sequential covering algorithm is to learn one rule, remove the data it covers, then repeat.
                            </p>
                            <p>
                                <img src="learning set of rules2.jpg" alt="learning set of rules">
                            </p>

                        </li>
                        <li>We require Learn-One-Rule to have high (perfect?) accuracy but not necessarily high coverage (i.e., when it makes a prediction it should be true).</li>
                        <li>Since it performs a greedy search it is not guaranteed to find the best or smallest set of rules that cover the training examples.</li>
                        <li>
                            <h4>3.1 Learn-One-Rule</h4>
                            <p>
                                <img src="learning set of rules3.jpg" alt="learning set of rules">
                            </p>
                            <ul>
                                <li>Idea: organize the hypothesis space search in general to specific fashion.</li>
                                <li>Start with most general rule precondition, then greedily add the attribute that most improves performance measured over the training examples.</li>
                                <li>
                                    <h5>3.1.1 Learn-One-Rule Algorithm</h5>
                                    <p>
                                        <img src="learning set of rules4.jpg" alt="learning set of rules">
                                    </p>
                                </li>
                                <li>
                                    <h5>3.1.2 Learn-One-Rule Example</h5>
                                    <p>
                                        <img src="learning set of rules5.jpg" alt="learning set of rules" width="1000px">
                                    </p>
                                </li>
                                <li>
                                    <h5>3.1.3 Learn One Summary</h5>
                                    <ul>
                                        <li>Can be generalized to multi-valued target functions.</li>
                                        <li>There are other ways to define Performance(), besides using entropy.</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <h4>3.2 CN2</h4>
                            <ul>
                                <li>
                                    <p>
                                        The CN2 [5] system by Clark and Niblett uses a subroutine similar to Learn-One-Rule. CN2 is described by the authors as
                                    </p>
                                    <p>
                                        <img src="learning set of rules7.jpg" alt="learning set of rules">
                                    </p>
                                </li>
                                <li>As such, CN2 is a sequential covering algorithm.</li>
                            </ul>
                        </li>
                        <li>
                            <h4>3.3 Other Variations</h4>
                            <ul>
                                <li>Might want to learn only rules that cover positive examples and include a default negative classification—good when there are very few positive.</li>
                                <li>Learn-One-Rule can be modified to accept argument that specifies target value of interest.</li>
                                <li>
                                    AQ algorithm learns a disjunctive set of rules that together cover the target function, but
                                    <ul>
                                        <li>seeks rules that cover a particular target value</li>
                                        <li>and does a beam search but using a single positive example to focus this search.</li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li>
                            <h4>3.4 Other Performance Measures</h4>
                            <ul>
                                <li>How to gage performance?</li>
                            </ul>
                            <ol>
                                <li>Relative frequency: Let n be the number of examples the rule matches and n c be the number that it classifies correctly. n c n</li>
                                <li>m-estimate of accuracy: Let p be the prior probability that a random example will be correctly classified correctly, let m be the weight. n c +mpn +m</li>
                                <li>Entropy: Let S be the set of examples that match the rule precondition, c be the number of distinct values the target function make take on, and p i the proportion of examples for which the target function takes the i th value - Entropy(S)=∑ i =1 cp ilog 2p i</li>
                            </ol>
                        </li>
                    </ul>
                </li>
           
        </div>
        <div id="footer">
            Made with <span style="color: rgb(243, 52, 148);"> &#10084; </span> by IS third year students.
        </div>
    </body>
</html>